# -*- coding: utf-8 -*-
"""FYP2_(Final).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ti68rKugWimWWRN-a22d-oNTqScDG6m6
"""

import pandas as pd
import numpy as np

from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

import keras
from keras import layers
from keras import ops

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

"""# Import cleaned csv from google drive"""

from google.colab import drive

drive.mount('/content/drive')

ls drive/MyDrive/cleaned_appliance.csv

appliance_df = pd.read_csv('/content/drive/My Drive/cleaned_appliance.csv', index_col=0)

appliance_df.count()

appliance_df.head(3)

"""# Content based filtering"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TfidfVectorizer and Remove stopwords
tfidf = TfidfVectorizer(stop_words='english')
# Fit and transform the data to a tfidf matrix
tfidf_matrix = tfidf.fit_transform(appliance_df['item_description']) #one min to compute
# Print the shape of the tfidf_matrix
tfidf_matrix.shape

from sklearn.metrics.pairwise import cosine_similarity

# Compute the cosine similarity between each movie description
#cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

def get_recommendations(title, cosine_sim=cosine_sim, num_recommend = 10):
    idx = indices[title]
# Get the pairwsie similarity scores of all movies with that movie
    sim_scores = list(enumerate(cosine_sim[idx]))
# Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
# Get the scores of the 10 most similar movies
    top_similar = sim_scores[1:num_recommend+1]
# Get the movie indices
    movie_indices = [i[0] for i in top_similar]
# Return the top 10 most similar movies
    return appliance_df['title'].iloc[movie_indices]

get_recommendations('Power Rangers Zeo', num_recommend = 20)

asins_to_lookup = ['B001RIYCK8', 'B0876V5MX6', 'B00FXYDIU2', 'B075M1F453', 'B00QAMO60C']

filtered_df = meta_df[meta_df['parent_asin'].isin(asins_to_lookup)]

for asin in asins_to_lookup:
    row = filtered_df[filtered_df['parent_asin'] == asin]
    if not row.empty:
        print(f"Parent ASIN: {asin}, Main Category: {row['main_category'].values[0]}")
    else:
        print(f"Parent ASIN: {asin} not found in the DataFrame.")

"""# Collaborative Filtering"""

user_ids = appliance_df["user_id"].unique().tolist()
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
userencoded2user = {i: x for i, x in enumerate(user_ids)}
item_ids = appliance_df["item_id"].unique().tolist()
item2item_encoded = {x: i for i, x in enumerate(item_ids)}
item_encoded2item = {i: x for i, x in enumerate(item_ids)}
appliance_df["user"] = appliance_df["user_id"].map(user2user_encoded)
appliance_df["item"] = appliance_df["item_id"].map(item2item_encoded)

num_users = len(user2user_encoded)
num_items = len(item_encoded2item)
appliance_df["user_rating"] = appliance_df["user_rating"].values.astype(np.float32)
#normalize user rating
min_rating = min(appliance_df["user_rating"])
max_rating = max(appliance_df["user_rating"])

print(
    "Number of users: {}, Number of Appliances: {}, Min rating: {}, Max rating: {}".format(
        num_users, num_items, min_rating, max_rating
    )
)

appliance_df = appliance_df.sample(frac=1, random_state=42) #random state can change
x = appliance_df[["user", "item"]].values
#normalize target to 1-0
y = appliance_df["user_rating"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
#trainset 90%, valset 10%
train_indices = int(0.9 * appliance_df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)

embedding_size = 128 #test for embedding, add another objecttive, know the best embedding size, 2^5->2^9

class RecommendationModel(keras.Model):
    def __init__(self, num_users, num_items, embedding_size, **kwargs):
        super().__init__(**kwargs)
        self.num_users = num_users
        self.num_items = num_items
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.item_embedding = layers.Embedding(
            num_items,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.item_bias = layers.Embedding(num_items, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        item_vector = self.item_embedding(inputs[:, 1])
        item_bias = self.item_bias(inputs[:, 1])
        dot_user_item = ops.tensordot(user_vector, item_vector, 2)
        # Add all the components (including bias)
        x = dot_user_item + user_bias + item_bias
        # The sigmoid activation forces the rating to between 0 and 1
        return ops.nn.sigmoid(x)


model = RecommendationModel(num_users, num_items, embedding_size)
model.compile(
    loss=keras.losses.BinaryCrossentropy(), #best result= the one with minimal lost
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
)

# Define the callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',      # Monitor validation loss
    patience=3,              # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restore the weights of the best model after stopping
)

model_checkpoint = ModelCheckpoint(
    filepath='best_model4.keras',  # Filepath to save the model
    monitor='val_loss',        # Monitor validation loss
    save_best_only=True,       # Save only the best model (lowest val_loss)
    verbose=1                  # Output messages when saving the model
)

# Fit the model with the callbacks
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=128,
    epochs=10,
    steps_per_epoch=5000,
    verbose=1,
    validation_data=(x_val, y_val),
    callbacks=[early_stopping, model_checkpoint]  # Add the callbacks here
)

model.summary()

#plot loss diagram
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

"""#Generate Top 10 Recommendation"""

# Let us get a user and see the top recommendations.
user_id = appliance_df.user_id.sample(1).iloc[0]
items_bought_by_user = appliance_df[appliance_df.user_id == user_id]
items_not_bought = appliance_df[
    ~appliance_df["user_id"].isin(items_bought_by_user.item_id.values)
]["item_id"]
items_not_bought = list(
    set(items_not_bought).intersection(set(item2item_encoded.keys()))
)
items_not_bought = [[item2item_encoded.get(x)] for x in items_not_bought]
user_encoder = user2user_encoded.get(user_id)
user_item_array = np.hstack(
    ([[user_encoder]] * len(items_not_bought), items_not_bought)
)
ratings = model.predict(user_item_array).flatten()
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_item_ids = [
    item2item_encoded.get(items_not_bought[x][0]) for x in top_ratings_indices
]

print("Showing recommendations for user: {}".format(user_id))
print("====" * 9)
print("Appliances with high ratings from user")
print("----" * 8)
top_items_user = (
    items_bought_by_user.sort_values(by="user_rating", ascending=False)
    .head(5)
    .item_id.values
)
movie_df_rows = appliance_df[appliance_df["item_id"].isin(top_items_user)]
for row in movie_df_rows.itertuples():
    print("Category :", row.item_main_category , ", Appliance title:", row.item_title)

print("----" * 8)
print("Top 10 Appliances recommendations")
print("----" * 8)
recommended_items = appliance_df[appliance_df["item_id"].isin(recommended_item_ids)]
for row in recommended_items.head(10).itertuples():
    print("Category :", row.item_main_category, ", Appliance title:", row.item_title)

"""# TESTING"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from surprise import Dataset, Reader, SVD

#Content-Based Filtering
content_df = data[['Product ID', 'Product Name', 'Brand',
                   'Category', 'Color', 'Size']]
content_df['Content'] = content_df.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)

# Use TF-IDF vectorizer to convert content into a matrix of TF-IDF features
tfidf_vectorizer = TfidfVectorizer()
content_matrix = tfidf_vectorizer.fit_transform(content_df['Content'])

content_similarity = linear_kernel(content_matrix, content_matrix)

reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(data[['User ID',
                                  'Product ID',
                                  'Rating']], reader)

def get_content_based_recommendations(product_id, top_n):
    index = content_df[content_df['Product ID'] == product_id].index[0]
    similarity_scores = content_similarity[index]
    similar_indices = similarity_scores.argsort()[::-1][1:top_n + 1]
    recommendations = content_df.loc[similar_indices, 'Product ID'].values
    return recommendations

#MAKE SURE ONLY RUN ONCE
#encode user id
user_ids = appliance_df["user_id"].unique().tolist()
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
userencoded2user = {i: x for i, x in enumerate(user_ids)}

#encode item id
item_ids = appliance_df["item_id"].unique().tolist()
item2item_encoded = {x: i for i, x in enumerate(item_ids)}
item_encoded2item = {i: x for i, x in enumerate(item_ids)}

#save to encoded id to df
appliance_df["user"] = appliance_df["user_id"].map(user2user_encoded)
appliance_df["item"] = appliance_df["item_id"].map(item2item_encoded)

#save number of encoded users and items
num_users = len(user2user_encoded)
num_items = len(item_encoded2item)

#normalize user ratings to [0,1] for loss functions - binary cross entropy
appliance_df["user_rating"] = appliance_df["user_rating"].values.astype(np.float32)
min_rating = min(appliance_df["user_rating"])
max_rating = max(appliance_df["user_rating"])

#print number of users, items and min and max of rating value
print(
    "Number of users: {}, Number of Appliances: {}, Min rating: {}, Max rating: {}".format(
        num_users, num_items, min_rating, max_rating
    )
)

# Extract features from text descriptions
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(appliance_df['item_description'])

tfidf_matrix.shape

# Compute the cosine similarity between each movie description
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
import pandas as pd

# Example item descriptions DataFrame
# Make sure your DataFrame has 'item_id' and 'description' columns
item_features_df = pd.DataFrame({
    'item_id': [1, 2, 3],
    'description': [
        'High quality smart thermostat',
        'Energy efficient air conditioner',
        'Durable washing machine with multiple features'
    ]
})

# Compute TF-IDF matrix
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(item_features_df['description'])

# Compute cosine similarity matrix
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

# Create a mapping from item_id to index
item_indices = pd.Series(item_features_df.index, index=item_features_df['item_id']).to_dict()

appliance_df = appliance_df.sample(frac=1, random_state=42) #random state can change
x = appliance_df[["user", "item"]].values
#normalize target to 1-0
y = appliance_df["user_rating"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
#trainset 90%, valset 10%
train_indices = int(0.9 * appliance_df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)

embedding_size = 32

class RecommendationModel(keras.Model):
    def __init__(self, num_users, num_items, num_item_features, embedding_size, **kwargs):
        super().__init__(**kwargs)
        self.num_users = num_users
        self.num_items = num_items
        self.num_item_features = num_item_features
        self.embedding_size = embedding_size

        # Collaborative filtering embeddings
        self.user_embedding = layers.Embedding(
            num_users, embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.item_embedding = layers.Embedding(
            num_items, embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.item_bias = layers.Embedding(num_items, 1)

        # Content-based filtering embeddings (for item features)
        self.item_feature_embedding = layers.Embedding(
            num_item_features, embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )

    def call(self, inputs):
        # Collaborative filtering: user and item embeddings
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        item_vector = self.item_embedding(inputs[:, 1])
        item_bias = self.item_bias(inputs[:, 1])

        # Content-based filtering: item features embedding
        item_feature_vector = self.item_feature_embedding(inputs[:, 2])

        # Combine item vectors (collaborative + content-based)
        combined_item_vector = layers.Concatenate()([item_vector, item_feature_vector])

        # Dot product of user and item vectors
        dot_user_item = tf.tensordot(user_vector, combined_item_vector, 2)

        # Add biases and apply sigmoid activation
        x = dot_user_item + user_bias + item_bias
        return tf.nn.sigmoid(x)

# Create the model
model = RecommendationModel(num_users, num_items, num_item_features, EMBEDDING_SIZE)
model.compile(
    loss=keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
)

# Define callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)
model_checkpoint = ModelCheckpoint(
    filepath='best_model.keras',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

def recommend_items(user_id, test_df, model, num_recommend=10):
    if user_id in test_df['user_id'].values:
        return get_model_based_recommendations(user_id, model, num_recommend)
    else:
        # Fallback to content-based filtering if no user rating exists
        sample_item_id = test_df['item_id'].iloc[0]  # Use a sample item ID
        return get_content_based_recommendations(sample_item_id, num_recommend)